{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "680d46aa-2094-43e9-be60-cb6a0881952a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras.optimizers\n",
    "import tensorflow.keras.metrics\n",
    "from tensorflow.keras import backend as K\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pickle\n",
    "import cartopy\n",
    "import cartopy.crs as ccrs\n",
    "import cartopy.feature as cf\n",
    "from datetime import date, datetime, timedelta\n",
    "import netCDF4 as nc\n",
    "from scipy.interpolate import griddata\n",
    "\n",
    "import glob\n",
    "import calendar\n",
    "import os,sys\n",
    "import rasterio, time\n",
    "import argparse\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import math\n",
    "import u_interpolate as uinterp\n",
    "from osgeo import gdal\n",
    "\n",
    "# Define domain and time period\n",
    "# date is supposed to change automatically\n",
    "\n",
    "\n",
    "# core path (most recent data)\n",
    "dataDir =\"/mnt/scratch/stewells/MSG_NRT/cut/\"\n",
    "# Geotiff dir on the SAN for portal\n",
    "SANdir = '/mnt/HYDROLOGY_stewells/geotiff/ssa_nowcast_cores_unet/'\n",
    "# testdate for default historical mode date  (to duplicate date used on https://github.com/JawairiaAA/WISER_testbed_2025/ )\n",
    "testdate='202501172100'\n",
    "archiveDir= \"/mnt/prj/nflics/cnn_cores/geotiff/\"\n",
    "\n",
    "# get the command arguments\n",
    "parser= argparse.ArgumentParser(description=\"Generate geotiffs of Nowcast cores\")\n",
    "\n",
    "# mode (default realtime)\n",
    "# mode : realtime = pick most recently added file that has not already been processed\n",
    "#        historical = pick a specific date TODO: allow for range of dates\n",
    "parser.add_argument(\"--mode\", choices=[\"realtime\",\"historical\"], default=\"realtime\",help=\"Run mode (real time or historical)\")\n",
    "parser.add_argument(\"--hDate\", type=str,default=testdate, help=\"Historical date to process (YYYYMMDDhhmm).\")\n",
    "# output directory, to be used if different from the SAN. Defaults to SAN\n",
    "parser.add_argument(\"--outDir\", type=str, default = SANdir, help=\"Directory to send outputs to (defaults to SAN)\")\n",
    "# load them\n",
    "args = parser.parse_args()\n",
    "#get variables from arguments\n",
    "mode = args.mode\n",
    "outRoot = args.outDir\n",
    "\n",
    "# lead times (hours) \n",
    "leadtimes = [1,2,4,6]\n",
    "\n",
    "\n",
    "\n",
    "# realtime: get most recently added file\n",
    "if mode=='realtime':\n",
    "    total_files=glob.glob(os.path.join(dataDir,'IR_108*'))\n",
    "    new_dates = []\n",
    "    for f in total_files:\n",
    "            modTimesinceEpoc = os.path.getmtime(f)\n",
    "\n",
    "            modificationTime = datetime.fromtimestamp(time.mktime(time.localtime(modTimesinceEpoc)))\n",
    "            if modificationTime > datetime.today()-timedelta(minutes=1):\n",
    "                idate =''.join(os.path.basename(f).split('_')[3:5]).split('.')[0]\n",
    "                # only process if not already done so (check existence of 6hr image - the last to process)\n",
    "                if not os.path.exists(os.path.join(outRoot,idate[:8],'nowcast_cores_unet_'+idate[0:8]+'_'+idate[8:12]+'_'+str(6)+'hr_3857.tif')):\n",
    "                    new_dates.append(idate)    \n",
    "\n",
    "    # Choose latest date only for now\n",
    "    if len(new_dates)>0:\n",
    "        current_date = new_dates[0]\n",
    "    else:\n",
    "        print(\"No data to process\")\n",
    "        sys.exit(0)\n",
    "elif mode== 'historical':\n",
    "    current_date = args.hDate\n",
    "    #current_date = '202501172100'\n",
    "\n",
    "print(\"Processing: \"+str(current_date))\n",
    "#--------------------------------\n",
    "current_year = current_date[0:4]\n",
    "current_month = current_date[4:6]\n",
    "current_day = current_date[6:8]\n",
    "t = 3 #numOfDays\n",
    "\n",
    "# cords of interest\n",
    "start_lat = -20 #5 # 5 \n",
    "end_lat = -5 #21 #10\n",
    "start_lon = 25 #-10\n",
    "end_lon = 55 #0\n",
    "\n",
    "# get native MSG grid (core)\n",
    "#coords_filename = glob.glob('/mnt/prj/Africa_cloud/geoloc/*.npz')[0]  # this is /prj/Africa_cloud/geoloc/*.npz on the Linux system\n",
    "coords_filename = glob.glob('/home/stewells/AfricaNowcasting/ancils/unet/lat_lon*.npz')[0]  # this is /prj/Africa_cloud/geoloc/*.npz on the Linux system\n",
    "\n",
    "msg_latlon = np.load(coords_filename)\n",
    "mlon = msg_latlon['lon']\n",
    "mlat = msg_latlon['lat']\n",
    "\n",
    "# find core indices using one file\n",
    "lat_ind = np.where((mlat[:,1]>=start_lat) & (mlat[:,1]<=end_lat))[0]\n",
    "lon_ind = np.where((mlon[1,:]>=start_lon) & (mlon[1,:]<=end_lon))[0]\n",
    "lat = mlat[lat_ind[0]:lat_ind[-1]+1,lon_ind[0]:lon_ind[-1]+1]\n",
    "lon = mlon[lat_ind[0]:lat_ind[-1]+1,lon_ind[0]:lon_ind[-1]+1]\n",
    "\n",
    "num_frames= 3   # \n",
    "t0= 1  #1   \n",
    "a= 11\n",
    "b= -25\n",
    "\n",
    "lon_sub = lon[a:,:b]\n",
    "lat_sub = lat[a:,:b]\n",
    "\n",
    "# prepare resampling\n",
    "dx = 0.026949456\n",
    "lat_min, lat_max= np.nanmin(lat_sub),np.nanmax(lat_sub)\n",
    "lon_min, lon_max= np.nanmin(lon_sub),np.nanmax(lon_sub)\n",
    "grid_lat = np.arange(lat_min,lat_max ,dx)\n",
    "grid_lon = np.arange(lon_min,lon_max ,dx)\n",
    "grid_lon, grid_lat = np.meshgrid(grid_lon,grid_lat)\n",
    "inds, weights, new_shape=uinterp.interpolation_weights(lon_sub[np.isfinite(lon_sub)], lat_sub[np.isfinite(lat_sub)],grid_lon, grid_lat, irregular_1d=True)\n",
    "\n",
    "\n",
    "\n",
    "def _create_mean_filter(half_num_rows, half_num_columns, num_channels):\n",
    "    \"\"\"Creates convolutional filter that computes mean.\n",
    "\n",
    "    M = number of rows in filter\n",
    "    N = number of columns in filter\n",
    "    C = number of channels\n",
    "\n",
    "    :param half_num_rows: Number of rows on either side of center.  This is\n",
    "        (M - 1) / 2.\n",
    "    :param half_num_columns: Number of columns on either side of center.  This\n",
    "        is (N - 1) / 2.\n",
    "    :param num_channels: Number of channels.\n",
    "    :return: weight_matrix: M-by-N-by-C-by-C numpy array of filter weights.\n",
    "    \"\"\"\n",
    "\n",
    "    num_rows = 2 * half_num_rows + 1\n",
    "    num_columns = 2 * half_num_columns + 1\n",
    "    weight = 1. / (num_rows * num_columns)\n",
    "\n",
    "    return np.full(\n",
    "        (num_rows, num_columns, num_channels, num_channels), weight,\n",
    "        dtype=np.float32\n",
    "    )\n",
    "\n",
    "def FSS_loss(target_tensor, prediction_tensor):\n",
    "    \n",
    "    half_window_size_px=2\n",
    "    use_as_loss_function=True \n",
    "    #mask_matrix\n",
    "    function_name=None\n",
    "    test_mode=False\n",
    "    \"\"\"Fractions skill score (FSS).\n",
    "\n",
    "    M = number of rows in grid\n",
    "    N = number of columns in grid\n",
    "\n",
    "    :param half_window_size_px: Number of pixels (grid cells) in half of\n",
    "        smoothing window (on either side of center).  If this argument is K, the\n",
    "        window size will be (1 + 2 * K) by (1 + 2 * K).\n",
    "    :param use_as_loss_function: Boolean flag.  FSS is positively oriented\n",
    "        (higher is better), but if using it as loss function, we want it to be\n",
    "        negatively oriented.  Thus, if `use_as_loss_function == True`, will\n",
    "        return 1 - FSS.  If `use_as_loss_function == False`, will return just\n",
    "        FSS.\n",
    "    :param mask_matrix: M-by-N numpy array of Boolean flags.  Grid cells marked\n",
    "        \"False\" are masked out and not used to compute the loss.\n",
    "    :param function_name: Function name (string).\n",
    "    :param test_mode: Leave this alone.\n",
    "    :return: loss: Loss function (defined below).\n",
    "    \"\"\"\n",
    "\n",
    "    weight_matrix = _create_mean_filter(\n",
    "        half_num_rows=half_window_size_px,\n",
    "        half_num_columns=half_window_size_px, num_channels=1\n",
    "    )\n",
    "       \n",
    "    \"\"\"Computes loss (fractions skill score).\n",
    "\n",
    "        :param target_tensor: Tensor of target (actual) values.\n",
    "        :param prediction_tensor: Tensor of predicted values.\n",
    "        :return: loss: Fractions skill score.\n",
    "    \"\"\"\n",
    "\n",
    "    smoothed_target_tensor = K.conv2d(\n",
    "        x=target_tensor, kernel=weight_matrix,\n",
    "        padding='same', strides=(1, 1), data_format='channels_last'\n",
    "    )\n",
    "\n",
    "    smoothed_prediction_tensor = K.conv2d(\n",
    "        x=prediction_tensor, kernel=weight_matrix,\n",
    "        padding='same', strides=(1, 1), data_format='channels_last'\n",
    "    )\n",
    "\n",
    "    actual_mse = K.mean(\n",
    "        (smoothed_target_tensor - smoothed_prediction_tensor) ** 2\n",
    "    )\n",
    "    reference_mse = K.mean(\n",
    "        smoothed_target_tensor ** 2 + smoothed_prediction_tensor ** 2\n",
    "    )\n",
    "\n",
    "    if use_as_loss_function:\n",
    "        return actual_mse / reference_mse\n",
    "\n",
    "    return 1. - actual_mse / reference_mse\n",
    "\n",
    "    if function_name is not None:\n",
    "        loss.__name__ = function_name\n",
    "\n",
    "\n",
    "def spatial_filter_conv(predicted_image):\n",
    "    \n",
    "    half_window_size_px=2\n",
    "    weight_matrix = _create_mean_filter(\n",
    "        half_num_rows=half_window_size_px,\n",
    "        half_num_columns=half_window_size_px, num_channels=1\n",
    "    )\n",
    "\n",
    "    smoothed_predicted_image = K.conv2d(\n",
    "        x=predicted_image, kernel=weight_matrix,\n",
    "        padding='same', strides=(1, 1), data_format='channels_last'\n",
    "    )\n",
    "    return smoothed_predicted_image\n",
    "\n",
    "\n",
    "from tensorflow.python.keras.engine import data_adapter\n",
    "\n",
    "def _is_distributed_dataset(ds):\n",
    "    return isinstance(ds, data_adapter.input_lib.DistributedDatasetSpec)\n",
    "\n",
    "\n",
    "\n",
    "cores = np.zeros((t,len(lat[:,1]),len(lon[1,:])),dtype=float)\n",
    "tir = np.zeros((t,len(lat[:,1]),len(lon[1,:])),dtype=float)\n",
    "time_core = np.zeros((t)) \n",
    "data_adapter._is_distributed_dataset = _is_distributed_dataset\n",
    "tf.config.run_functions_eagerly(True)\n",
    "\n",
    "\n",
    "##### Define input shape\n",
    "image_height= 512\n",
    "image_width= 512   #\n",
    "num_channels= 3    #    \n",
    "\n",
    "\n",
    "\n",
    "# load files\n",
    "current_date_int = datetime.strptime(current_date, '%Y%m%d%H%M')\n",
    "to_date=datetime.strptime(str(current_date), '%Y%m%d%H%M')\n",
    "to_minus_1hr_date=current_date_int-timedelta(hours=1)\n",
    "to_minus_1hr_date= to_minus_1hr_date.strftime('%Y%m%d%H%M')\n",
    "to_minus_2hr_date=current_date_int-timedelta(hours=2)\n",
    "to_minus_2hr_date= to_minus_2hr_date.strftime('%Y%m%d%H%M')\n",
    "\n",
    "dates_of_interest = [to_minus_2hr_date,to_minus_1hr_date,str(current_date)]\n",
    "print(\"T0: \"+str(current_date))\n",
    "#dir_name = '/mnt/prj/nflics/real_time_data/'+current_year+'/'+current_month.zfill(2)+'/'+current_day.zfill(2)+'/' if mode=='historical' else dataDir\n",
    "dir_name = '/mnt/prj/nflics/real_time_data/'+current_year+'/'+current_month.zfill(2)+'/'+current_day.zfill(2)+'/' \n",
    "\n",
    "\n",
    "list_of_files=[]                \n",
    "for a in range(0,len(dates_of_interest),1):\n",
    "    dates_of_interest_curr = dates_of_interest[a]\n",
    "    list_of_files.append(dir_name+'IR_108_BT_'+dates_of_interest_curr[0:4]+dates_of_interest_curr[4:6]+dates_of_interest_curr[6:8]+'_'+dates_of_interest_curr[8:]+'_eumdat.nc')\n",
    "\n",
    "# check for existing t0-2 file\n",
    "\n",
    "if os.path.exists(list_of_files[0]) == False:\n",
    "    \n",
    "    list_of_files[0]=list_of_files[2]\n",
    "    list_of_files[1]=list_of_files[2] \n",
    "    \n",
    "    to2_date = dates_of_interest[0]\n",
    "    try:\n",
    "        dir_name = '/prj/nflics/real_time_data/'+current_year+'/'+to2_date[4:6]+'/'+to2_date[6:8]+'/' \n",
    "        all_file_names = sorted(glob.glob(dir_name+\"IR*.nc\"));  #\n",
    "        latest_to2_file = all_file_names[-4*2] \n",
    "        # check time between files \n",
    "        to_2_date=latest_to2_file[-23:-15]+latest_to2_file[-14:-10]\n",
    "        to_2_datetime=datetime.strptime(str(int(to_2_date)), '%Y%m%d%H%M')\n",
    "        time_difference = to_date-to_2_datetime    \n",
    "        if time_difference< timedelta(hours=2.1):\n",
    "            list_of_files[0]=latest_to2_file\n",
    "            list_of_files[1]=all_file_names[-4]\n",
    "        else:\n",
    "            list_of_files[0]=list_of_files[2]\n",
    "            list_of_files[1]=list_of_files[2]  \n",
    "    except:\n",
    "        print(\"Unable to find suitable replacement for T0-2. Using T0 at all three time steps\")\n",
    "        list_of_files[0]=list_of_files[2]\n",
    "        list_of_files[1]=list_of_files[2]  \n",
    "\n",
    "# read in tir data\n",
    "for l in range(0,len(list_of_files),1): \n",
    "    tir_filename = list_of_files[l]\n",
    "    if os.path.exists(tir_filename):\n",
    "        ds = xr.open_dataset(tir_filename).squeeze() \n",
    "        tir_temp =  ds['ir108_bt'].values  #/10000\n",
    "        tir[l,:,:] = tir_temp[lat_ind[0]:lat_ind[-1]+1,lon_ind[0]:lon_ind[-1]+1]   \n",
    "        ds = None \n",
    "    time_core[l] = int(dates_of_interest[l])  #int(tir_filename[-15:-3])\n",
    "\n",
    "\n",
    "num_frames= 3   # \n",
    "t0= 1  #1   \n",
    "a= 11\n",
    "b= -25\n",
    "\n",
    "# loop over lead times here\n",
    "for leadtime in leadtimes:\n",
    "\n",
    "    print(\"Processing \"+str(leadtime)+'hr leadtime')\n",
    "    ind = np.where(cores>0)\n",
    "    cores[ind] = 1 \n",
    "    cores_t_0 = cores[:,a:,:b]\n",
    "    tir_t_0 = tir[:,a:,:b]\n",
    "    ind_tir = np.where(tir_t_0>-0.01)\n",
    "    tir_t_0[ind_tir] = 0\n",
    "    tir_t_0[np.isnan(tir_t_0)] = 0\n",
    "    tir_t_0 = np.round(tir_t_0/-173,4)\n",
    "\n",
    "\n",
    "    modelFile= '/home/stewells/AfricaNowcasting/ancils/unet/'+str(leadtime)+'hr_using_1hr/ZA_Jan_Feb_trained_model_2005_to_2019.h5'\n",
    "    print(os.path.exists(modelFile))\n",
    "    unet_model = tf.keras.models.load_model(modelFile, compile=False,custom_objects={'loss': FSS_loss})\n",
    "\n",
    "    unet_model.compile(optimizer=tensorflow.keras.optimizers.Adam(),\n",
    "                    loss=FSS_loss,\n",
    "                    metrics=[tf.keras.metrics.Accuracy()])\n",
    "\n",
    "\n",
    "    #prediction_time = time_core*0\n",
    "    #for i in range(0,len(time_core)):\n",
    "    #    time_core_dt = datetime.strptime(dates_of_interest[i], '%Y%m%d%H%M')\n",
    "    #    prediction_time_temp = (time_core_dt+timedelta(hours=leadtime)).strftime('%Y%m%d%H%M')\n",
    "    #    prediction_time[i] = int(prediction_time_temp)\n",
    "    \n",
    "    prediction_time = int((to_date+timedelta(hours=leadtime)).strftime('%Y%m%d%H%M'))\n",
    "\n",
    "    # Define input shape\n",
    "    image_height= len(tir_t_0[1,:,1]) #lat\n",
    "    image_width= len(tir_t_0[1,1,:]) #lon\n",
    "    num_channels= 3 #  core at t0-, core at t0-1,  \n",
    "\n",
    "    x_pred= np.zeros((1,image_height,image_width, num_channels))\n",
    "    x_pred[:,:,:,0]= tir_t_0[0,:]\n",
    "    x_pred[:,:,:,1]= tir_t_0[1,:]\n",
    "    x_pred[:,:,:,2]= tir_t_0[2,:]\n",
    "\n",
    "    time_of_day_pred= np.zeros((1,image_height,image_width,1))\n",
    "    #time_of_day = float(str(prediction_time[-1])[-6:])/2345\n",
    "    time_of_day = float(str(prediction_time)[-6:])/2345\n",
    "    time_of_day_pred[:,:,:,:]=np.round(np.sin(time_of_day*math.pi),2)\n",
    "\n",
    "    #predicted_frames= np.round(np.squeeze(unet_model.predict([x_pred,time_of_day_pred])),2)\n",
    "   \n",
    "    \n",
    "    predicted_frames= np.round(unet_model.predict([x_pred,time_of_day_pred]),2)\n",
    "    \n",
    "    filtered_image = spatial_filter_conv(predicted_frames)\n",
    "    filtered_image = np.squeeze(filtered_image[0,:,:,0])\n",
    "\n",
    "#resample\n",
    "    data_interp=uinterp.interpolate_data(filtered_image, inds, weights, new_shape)\n",
    "# convert to probability\n",
    "    data_interp*=100.\n",
    "    # save geotiff\n",
    "    # temporary file in original EPSG (to be deleted once converted to 3857 for portal)\n",
    "    rasFile_tmp = '/mnt/HYDROLOGY_stewells/geotiff/ssa_nowcast_cores_unet/unet_'+str(leadtime)+'hr_tmp.tif'\n",
    "    outDir = os.path.join(outRoot,current_date[0:8])\n",
    "    os.makedirs(outDir,exist_ok=True)\n",
    "    rasFile =     os.path.join(outDir,'nowcast_cores_unet_'+current_date[0:8]+'_'+current_date[8:12]+'_'+str(leadtime)+'hr_3857.tif')\n",
    "\n",
    "    transform = rasterio.transform.from_origin(lon_min,lat_max,dx,dx)\n",
    "    dat_type = str(data_interp.dtype)\n",
    "    rasImage = rasterio.open(rasFile_tmp,'w',driver='GTiff',\n",
    "                            height=data_interp.shape[0],width=data_interp.shape[1],\n",
    "                            count=1,dtype=dat_type,\n",
    "                            crs = \"EPSG:4326\",#origEPSG\n",
    "                            nodata=-999.9,\n",
    "                            transform = transform                           \n",
    "                        )\n",
    "    #for ix,Image in enumerate(data):\n",
    "    rasImage.write(np.flipud(data_interp[:]),1)\n",
    "    rasImage.close()\n",
    "    archDir= os.path.join(archiveDir,current_date[0:8])\n",
    "    os.makedirs(archDir,exist_ok=True)\n",
    "    archFile =os.path.join(archDir,'nowcast_cores_unet_'+current_date[0:8]+'_'+current_date[8:12]+'_'+str(leadtime)+'hr_4326.tif')\n",
    "    try:\n",
    "        os.system('cp '+rasFile_tmp+' '+archFile)\n",
    "    except:\n",
    "        print(\"Failed to write to geotiff in 4326 archive directory\")\n",
    "    try:\n",
    "        # reproject onto EPSG:3857 for portal usage\n",
    "        ds = gdal.Warp(rasFile, rasFile_tmp, srcSRS='EPSG:4326', dstSRS='EPSG:3857', format='GTiff',creationOptions=[\"COMPRESS=DEFLATE\", \"TILED=YES\"])\n",
    "        ds = None \n",
    "    except:\n",
    "        print(\"Failed to write to output directory\")\n",
    "    os.system('rm '+rasFile_tmp)\n",
    "    \n",
    "    \n",
    "    archFile =os.path.join(archDir,'nowcast_cores_unet_'+current_date[0:8]+'_'+current_date[8:12]+'_'+str(leadtime)+'hr_3857.tif')\n",
    "    try:\n",
    "        os.system('cp '+rasFile+' '+archFile)\n",
    "    except:\n",
    "        print(\"Failed to write to geotiff archive directory\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "base"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
